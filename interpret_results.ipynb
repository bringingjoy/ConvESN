{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/datasets/home/50/950/jhamer/ConvESN/reservoir.py:7: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 1431, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-e22e2ef9eb36>\", line 4, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 71, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as cp\n",
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, concatenate\n",
    "from keras.layers import Conv2D, GlobalMaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import reservoir\n",
    "import utils\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e850ec987a09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cross_Validation_Results_DailyActivity.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_csv\u001b[0;34m(cls, path, header, sep, index_col, parse_dates, encoding, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[1;32m   1359\u001b[0m                           \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m                           \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtupleize_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m                           infer_datetime_format=infer_datetime_format)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'block'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "# results_df = pd.DataFrame.from_csv(\"Cross_Validation_Results_DailyActivity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results_df)\n",
    "# print(results_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average accuracy for each activity\n",
    "# print(\"Activity\\t\\tAverage accuracy\")\n",
    "# for key in results_df.keys():\n",
    "#     print(key, \"\\t\\t\", np.mean(results_df[key]), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = np.load(\"confusion_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Dimens of train labels (288,) dimens of test labels (32,)\n",
      "Transfering labels...\n",
      "Num training samples: 288\n",
      "Num testing samples: 32\n",
      "From training set: num samples 288 time_length 549 n_in 12\n",
      "Hyperparameters:\n",
      "Reservoir size: 96\n",
      "Input scale: 0.1\n",
      "Spectral radius: 0.99\n",
      "Sparsity: 0.1\n",
      "Leaking rate: 0.9\n",
      "Getting echo states...\n",
      "****************************************************************\n",
      "Evaluating the model on the test set:\n",
      "Loading in the saved model weights with best validation accuracy\n",
      "****************************************************************\n",
      "****************************************************************\n",
      "Overall classification accuracy:\n",
      "categorical_accuracy: 75.00%\n",
      "****************************************************************\n",
      "****************************************************************\n",
      "Prediction accuracies by activity:\n",
      "****************************************************************\n",
      "Activity\tAccuracy (% correct)\n",
      "drink: 100.0% correct\n",
      "eat: 100.0% correct\n",
      "read book: 50.0% correct\n",
      "call cellphone: 50.0% correct\n",
      "write on a paper: 50.0% correct\n",
      "use laptop: 50.0% correct\n",
      "use vacuum cleaner: 100.0% correct\n",
      "cheer up: 100.0% correct\n",
      "sit still: 50.0% correct\n",
      "toss paper: 100.0% correct\n",
      "play game: 50.0% correct\n",
      "lie down on sofa: 50.0% correct\n",
      "walk: 100.0% correct\n",
      "play guitar: 50.0% correct\n",
      "stand up: 100.0% correct\n",
      "sit down: 100.0% correct\n",
      "Dimens of train labels (288,) dimens of test labels (32,)\n",
      "Transfering labels...\n",
      "Num training samples: 288\n",
      "Num testing samples: 32\n",
      "From training set: num samples 288 time_length 549 n_in 12\n",
      "Hyperparameters:\n",
      "Reservoir size: 96\n",
      "Input scale: 0.1\n",
      "Spectral radius: 0.99\n",
      "Sparsity: 0.1\n",
      "Leaking rate: 0.9\n",
      "Getting echo states...\n",
      "****************************************************************\n",
      "Evaluating the model on the test set:\n",
      "Loading in the saved model weights with best validation accuracy\n",
      "****************************************************************\n",
      "****************************************************************\n",
      "Overall classification accuracy:\n",
      "categorical_accuracy: 9.38%\n",
      "****************************************************************\n",
      "****************************************************************\n",
      "Prediction accuracies by activity:\n",
      "****************************************************************\n",
      "Activity\tAccuracy (% correct)\n",
      "drink: 0.0% correct\n",
      "eat: 0.0% correct\n",
      "read book: 0.0% correct\n",
      "call cellphone: 0.0% correct\n",
      "write on a paper: 0.0% correct\n",
      "use laptop: 0.0% correct\n",
      "use vacuum cleaner: 0.0% correct\n",
      "cheer up: 0.0% correct\n",
      "sit still: 50.0% correct\n",
      "toss paper: 0.0% correct\n",
      "play game: 0.0% correct\n",
      "lie down on sofa: 100.0% correct\n",
      "walk: 0.0% correct\n",
      "play guitar: 0.0% correct\n",
      "stand up: 0.0% correct\n",
      "sit down: 0.0% correct\n",
      "Dimens of train labels (288,) dimens of test labels (32,)\n",
      "Transfering labels...\n",
      "Num training samples: 288\n",
      "Num testing samples: 32\n",
      "From training set: num samples 288 time_length 549 n_in 12\n",
      "Hyperparameters:\n",
      "Reservoir size: 96\n",
      "Input scale: 0.1\n",
      "Spectral radius: 0.99\n",
      "Sparsity: 0.1\n",
      "Leaking rate: 0.9\n",
      "Getting echo states...\n",
      "****************************************************************\n",
      "Evaluating the model on the test set:\n",
      "Loading in the saved model weights with best validation accuracy\n",
      "****************************************************************\n",
      "****************************************************************\n",
      "Overall classification accuracy:\n",
      "categorical_accuracy: 9.38%\n",
      "****************************************************************\n",
      "****************************************************************\n",
      "Prediction accuracies by activity:\n",
      "****************************************************************\n",
      "Activity\tAccuracy (% correct)\n",
      "drink: 0.0% correct\n",
      "eat: 0.0% correct\n",
      "read book: 0.0% correct\n",
      "call cellphone: 0.0% correct\n",
      "write on a paper: 50.0% correct\n",
      "use laptop: 50.0% correct\n",
      "use vacuum cleaner: 0.0% correct\n",
      "cheer up: 0.0% correct\n",
      "sit still: 50.0% correct\n",
      "toss paper: 0.0% correct\n",
      "play game: 0.0% correct\n",
      "lie down on sofa: 0.0% correct\n",
      "walk: 0.0% correct\n",
      "play guitar: 0.0% correct\n",
      "stand up: 0.0% correct\n",
      "sit down: 0.0% correct\n",
      "Dimens of train labels (288,) dimens of test labels (32,)\n",
      "Transfering labels...\n",
      "Num training samples: 288\n",
      "Num testing samples: 32\n",
      "From training set: num samples 288 time_length 549 n_in 12\n",
      "Hyperparameters:\n",
      "Reservoir size: 96\n",
      "Input scale: 0.1\n",
      "Spectral radius: 0.99\n",
      "Sparsity: 0.1\n",
      "Leaking rate: 0.9\n",
      "Getting echo states...\n",
      "****************************************************************\n",
      "Evaluating the model on the test set:\n",
      "Loading in the saved model weights with best validation accuracy\n",
      "****************************************************************\n",
      "****************************************************************\n",
      "Overall classification accuracy:\n",
      "categorical_accuracy: 12.50%\n",
      "****************************************************************\n",
      "****************************************************************\n",
      "Prediction accuracies by activity:\n",
      "****************************************************************\n",
      "Activity\tAccuracy (% correct)\n",
      "drink: 0.0% correct\n",
      "eat: 0.0% correct\n",
      "read book: 0.0% correct\n",
      "call cellphone: 0.0% correct\n",
      "write on a paper: 0.0% correct\n",
      "use laptop: 0.0% correct\n",
      "use vacuum cleaner: 0.0% correct\n",
      "cheer up: 0.0% correct\n",
      "sit still: 50.0% correct\n",
      "toss paper: 0.0% correct\n",
      "play game: 100.0% correct\n",
      "lie down on sofa: 50.0% correct\n",
      "walk: 0.0% correct\n",
      "play guitar: 0.0% correct\n",
      "stand up: 0.0% correct\n",
      "sit down: 0.0% correct\n",
      "Dimens of train labels (288,) dimens of test labels (32,)\n",
      "Transfering labels...\n",
      "Num training samples: 288\n",
      "Num testing samples: 32\n",
      "From training set: num samples 288 time_length 549 n_in 12\n",
      "Hyperparameters:\n",
      "Reservoir size: 96\n",
      "Input scale: 0.1\n",
      "Spectral radius: 0.99\n",
      "Sparsity: 0.1\n",
      "Leaking rate: 0.9\n",
      "Getting echo states...\n",
      "****************************************************************\n",
      "Evaluating the model on the test set:\n",
      "Loading in the saved model weights with best validation accuracy\n",
      "****************************************************************\n",
      "****************************************************************\n",
      "Overall classification accuracy:\n",
      "categorical_accuracy: 9.38%\n",
      "****************************************************************\n",
      "****************************************************************\n",
      "Prediction accuracies by activity:\n",
      "****************************************************************\n",
      "Activity\tAccuracy (% correct)\n",
      "drink: 0.0% correct\n",
      "eat: 50.0% correct\n",
      "read book: 0.0% correct\n",
      "call cellphone: 0.0% correct\n",
      "write on a paper: 0.0% correct\n",
      "use laptop: 0.0% correct\n",
      "use vacuum cleaner: 0.0% correct\n",
      "cheer up: 0.0% correct\n",
      "sit still: 0.0% correct\n",
      "toss paper: 0.0% correct\n",
      "play game: 50.0% correct\n",
      "lie down on sofa: 50.0% correct\n",
      "walk: 0.0% correct\n",
      "play guitar: 0.0% correct\n",
      "stand up: 0.0% correct\n",
      "sit down: 0.0% correct\n",
      "Accuracy averaged across all splits: 23.127999999999997\n",
      "CONFUSION MATRIX\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-679065ec3afd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CONFUSION MATRIX\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0mcnf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;31m# cp.dump(cm, open(\"confusion_matrix_\" + training_description + \".pkl\", \"wb\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cm' is not defined"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "   \n",
    "splits = np.arange(1, 6)\n",
    "# train_path = \"./data/MSRDailyActivity3D_train_split_s\"\n",
    "# test_path = \"./data/MSRDailyActivity3D_test_split_s\"\n",
    "train_path = \"./data/MSRDailyActivity3D_fromAction_noInterpolation_train_split_s\"\n",
    "test_path = \"./data/MSRDailyActivity3D_fromAction_noInterpolation_test_split_s\"\n",
    "\n",
    "# Map the numerical activity key to its description\n",
    "activity_dict = {1: \"drink\", 2: \"eat\", 3: \"read book\", 4: \"call cellphone\", \n",
    "                 5: \"write on a paper\", 6: \"use laptop\", 7: \"use vacuum cleaner\", \n",
    "                 8: \"cheer up\", 9: \"sit still\", 10: \"toss paper\", \n",
    "                 11: \"play game\", 12: \"lie down on sofa\", 13: \"walk\", \n",
    "                 14: \"play guitar\", 15: \"stand up\", 16: \"sit down\"}\n",
    "\n",
    "header = [\"HOLDOUT_SUBJECT\", \"HOLDOUT_ACC\"] #, \"HOLDOUT_LOSS\", \"TRAIN_LOSS\", \"TRAIN_ACC\"]\n",
    "\n",
    "# Write a descriptive header with the 16 activities\n",
    "for key in activity_dict:\n",
    "    header.append(activity_dict[key])\n",
    "\n",
    "\n",
    "testset_pred = []\n",
    "testset_true = []\n",
    "    \n",
    "with open(\"analysis_of_results_on_5_holdouts.csv\", 'w') as outcsv:\n",
    "    writer = csv.writer(outcsv)\n",
    "                   \n",
    "    writer.writerow(header)\n",
    "    \n",
    "    accuracy_across_splits = []\n",
    "    for split in splits:\n",
    "\n",
    "        # Load in the next training and holdout split\n",
    "    #     filepath = \"./data/\" + dataset_name + \"_P4_Split_\" + split #+ sys.argv[2]\n",
    "        data_train = cp.load(open(train_path + str(split) + \".pkl\", \"rb\"))\n",
    "        skeletons_train = data_train[0:5]\n",
    "        labels_train = data_train[5]\n",
    "\n",
    "        data_test = cp.load(open(test_path + str(split) + \".pkl\", \"rb\"))\n",
    "        skeletons_test = data_test[0:5]\n",
    "        labels_test = data_test[5]\n",
    "\n",
    "\n",
    "        print(\"Dimens of train labels\", labels_train.shape, \"dimens of test labels\", labels_test.shape)\n",
    "        print('Transfering labels...')\n",
    "        labels_train, labels_test, num_classes = utils.transfer_labels(labels_train, labels_test)\n",
    "\n",
    "        \"\"\"\n",
    "        Set parameters of reservoirs, create five reservoirs \n",
    "        and get echo states of five skeleton parts\n",
    "        \"\"\"\n",
    "        num_samples_train = labels_train.shape[0]\n",
    "    #     num_samples_valid = labels_valid.shape[0]\n",
    "        num_samples_test = labels_test.shape[0]\n",
    "\n",
    "        print(\"Num training samples:\", num_samples_train)\n",
    "    #     print(\"Num validation samples:\", num_samples_valid)\n",
    "        print(\"Num testing samples:\", num_samples_test)\n",
    "\n",
    "        # Total number of training samples, max number of frames across all train/test videos\n",
    "        num_samples, time_length, n_in = skeletons_train[0].shape\n",
    "        print(\"From training set: num samples\", num_samples, \"time_length\", time_length, \"n_in\", n_in)\n",
    "        # print(\"From test set: num samples, time length, and n_in:\", skeletons_test[0].shape)\n",
    "\n",
    "        # Original hyperparameter settings\n",
    "        n_res = n_in * 8\n",
    "        IS = 0.1\n",
    "        SR = 0.99 # 0.99 in the paper \n",
    "        sparsity = 0.1\n",
    "        leakyrate = 0.9\n",
    "\n",
    "        # Set hyperparameter optimization - random search - settings\n",
    "        # n_res = n_in * 3\n",
    "        # input_scaling = [0.001, 1]   # input scaling\n",
    "        # spectral_radius = [0.1, 1] # spectral radius of reservoir weight matrix (W_res)\n",
    "        # leaky_rate = [0.1, 1]       # leaky-integrated discrete-time continuous-value units\n",
    "        # sparsity = [0.0001, 0.5]\n",
    "\n",
    "        # input_scaling = np.linspace(input_scaling[0], input_scaling[1], 50)\n",
    "        # spectral_radius = np.linspace(spectral_radius[0], spectral_radius[1], 15)\n",
    "        # leaky_rate = np.linspace(leaky_rate[0], leaky_rate[1], 5)\n",
    "        # sparsity = np.linspace(sparsity[0], sparsity[1], 20)\n",
    "\n",
    "        print(\"Hyperparameters:\")\n",
    "        print(\"Reservoir size:\", n_res)\n",
    "        print(\"Input scale:\", IS)\n",
    "        print(\"Spectral radius:\", SR)\n",
    "        print(\"Sparsity:\", sparsity)\n",
    "        print(\"Leaking rate:\", leakyrate)\n",
    "\n",
    "\n",
    "        # Initialize five different reservoirs, one for each skeletal region\n",
    "        reservoirs = [reservoir.reservoir_layer(n_in, n_res, IS, SR, sparsity, leakyrate) for i in range(5)]\n",
    "        print('Getting echo states...')\n",
    "\n",
    "#         echo_states_train = [np.empty((num_samples_train, 1, time_length, n_res), np.float32) for i in range(5)]\n",
    "        echo_states_test = [np.empty((num_samples_test, 1, time_length, n_res), np.float32) for i in range(5)]\n",
    "\n",
    "        # Get the Echo States for the 5 skeletal regions for each dataset split\n",
    "        for i in range(5):\n",
    "#             echo_states_train[i][:, 0, :, :] = reservoirs[i].get_echo_states(skeletons_train[i])\n",
    "            echo_states_test[i][:, 0, :, :] = reservoirs[i].get_echo_states(skeletons_test[i])\n",
    "\n",
    "#         echo_states_train = [np.concatenate(echo_states_train[0:2], axis=1), \n",
    "#                              np.concatenate(echo_states_train[2:4], axis=1), echo_states_train[4]]\n",
    "        echo_states_test = [np.concatenate(echo_states_test[0:2], axis=1), \n",
    "                            np.concatenate(echo_states_test[2:4], axis=1), echo_states_test[4]]\n",
    "\n",
    "        \"\"\"\n",
    "        Set hyperparameters of convolution layers and build the MSMC decoder model\n",
    "        Hyperparameters include:\n",
    "        # epochs: 300\n",
    "        batch size: 8\n",
    "        # filters: 16\n",
    "        kernel stride: 1x1\n",
    "        conv kernel initialization: LeCun uniform\n",
    "        activation function: ReLU\n",
    "        SGD optimizer: Adam\n",
    "        \"\"\"\n",
    "        input_shapes = ((2, time_length, n_res), (2, time_length, n_res), (1, time_length, n_res))\n",
    "        nb_filter = 16\n",
    "        nb_row = (2, 3, 4) # Time scales\n",
    "        nb_col = n_res\n",
    "        kernel_initializer = 'lecun_uniform'\n",
    "        activation = 'relu' # Conv2D activation function; reservoirs use tanh\n",
    "        padding = 'valid'\n",
    "        strides = (1, 1)\n",
    "\n",
    "        data_format = 'channels_first'\n",
    "        optimizer = 'adam'\n",
    "        batch_size = 8\n",
    "        nb_epoch = 150\n",
    "        verbose = 2 # One line per epoch\n",
    "\n",
    "        # Build the MSMC decoder model\n",
    "        inputs = []\n",
    "        features = []\n",
    "        for i in range(3):\n",
    "            input = Input(shape=input_shapes[i])\n",
    "            inputs.append(input)\n",
    "\n",
    "            pools = []\n",
    "            for j in range(len(nb_row)):\n",
    "                conv = Conv2D(nb_filter, (nb_row[j], nb_col), \n",
    "                              kernel_initializer=kernel_initializer, activation=activation, \n",
    "                              padding=padding, strides=strides, data_format=data_format)(input)\n",
    "                pool = GlobalMaxPooling2D(data_format=data_format)(conv)\n",
    "                pools.append(pool)\n",
    "\n",
    "            features.append(concatenate(pools))\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        hands_features = features[0]\n",
    "        legs_features = features[1]\n",
    "        trunk_features = features[2]\n",
    "        body_features = Dense(nb_filter * len(nb_row), kernel_initializer = kernel_initializer, activation = activation)(concatenate([hands_features, legs_features, trunk_features]))\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the Conv component of the model\n",
    "        body_features = Dense(nb_filter * len(nb_row), kernel_initializer=kernel_initializer, \n",
    "                              activation=activation)(concatenate(features))\n",
    "\n",
    "        # Initialize the output layer of the model with softmax for classification\n",
    "        outputs = Dense(num_classes, kernel_initializer=kernel_initializer, activation='softmax')(body_features)\n",
    "\n",
    "        # Initialize the ConvESN_MSMC model using Adam optimizer, Categorical Cross-Ent loss, and accuracy evaluation metric\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "#         model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        # model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # model.fit(echo_states_train, labels_train, batch_size=batch_size, epochs=nb_epoch, \n",
    "        #           verbose=verbose, validation_data=(echo_states_test, labels_test))\n",
    "\n",
    "        # Save important training data: trained model weights, plot of training loss, training history\n",
    "        training_description = \"DailyActivity_asAction_\" + \"holdout_s\" + str(split)\n",
    "\n",
    "        # Setup up the EarlyStopping callback criteria\n",
    "#         early_stopping = EarlyStopping(monitor=monitor, min_delta=min_delta, patience=patience, \n",
    "#                                           mode=mode, baseline=baseline)\n",
    "\n",
    "        # Save the best model checkpoint only, based on validation accuracy\n",
    "        model_filepath = \"./model_chkpts/model_params\" + training_description\n",
    "#         model_checkpoint = ModelCheckpoint(model_filepath, monitor=\"val_categorical_accuracy\", verbose=1, \n",
    "#                                            save_best_only=True, mode=\"max\")\n",
    "                                                                             \n",
    "#         callbacks_list = [early_stopping, model_checkpoint] # [model_checkpoint] \n",
    "\n",
    "        # Train and evaluate the model on training and validation sets, using EARLY STOPPING\n",
    "#         history = model.fit(echo_states_train, labels_train, batch_size=batch_size, epochs=nb_epoch, verbose=verbose, \n",
    "#                             callbacks=callbacks_list, validation_data=(echo_states_test, labels_test))\n",
    "\n",
    "\n",
    "    #     model.save_weights(\"./model_chkpts/model_params\" + training_description)\n",
    "    #     plt.plot(history.history[\"loss\"])\n",
    "    #     plt.savefig(training_description + \"_training_loss_plot.jpg\")\n",
    "    #     plt.plot(history.history[\"val_loss\"])\n",
    "    #     plt.savefig(training_description + \"_val_loss_plot.jpg\")\n",
    "\n",
    "        # Save the training history as a pickled dictionary\n",
    "#         cp.dump(history.history, open(training_description + \"_training_history_dict.pkl\", \"wb\"))\n",
    "\n",
    "        print(\"****************************************************************\")\n",
    "        print(\"Evaluating the model on the test set:\")\n",
    "        print(\"Loading in the saved model weights with best validation accuracy\")\n",
    "        print(\"****************************************************************\")\n",
    "        # Load in saved weights and recompile model\n",
    "        model.load_weights(model_filepath)\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "        # Get the echo states on the test set and evaluate the model on the test set\n",
    "        scores = model.evaluate(x=echo_states_test, y=labels_test, batch_size=batch_size, verbose=verbose)\n",
    "        print(\"****************************************************************\")\n",
    "        print(\"Overall classification accuracy:\")\n",
    "        overall_acc = round(scores[1]*100, 2)\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    #     print(\"Test set loss:\")\n",
    "    #     print(\"%s: %.2f%\" % (model.metrics_names[0], scores[0]))\n",
    "        print(\"****************************************************************\")\n",
    "\n",
    "        # Compute the model predictions and print the accuracies by activity\n",
    "        predictions = model.predict(x=echo_states_test)\n",
    "\n",
    "        accuracies = np.zeros((num_classes))\n",
    "        totals_per_class = np.zeros((num_classes))\n",
    "\n",
    "        for i in range(len(predictions)):\n",
    "\n",
    "            pred_label = np.argmax(predictions[i])\n",
    "            true_label = np.argmax(labels_test[i])\n",
    "            if pred_label == true_label:\n",
    "                accuracies[true_label] += 1\n",
    "            totals_per_class[true_label] += 1\n",
    "            # Add the prediction and ground truth to lists\n",
    "            testset_pred.append(pred_label)\n",
    "            testset_true.append(true_label)\n",
    "            \n",
    "        print(\"****************************************************************\")\n",
    "        print(\"Prediction accuracies by activity:\")\n",
    "        print(\"****************************************************************\")\n",
    "\n",
    "#         log_file = open(\"./Results_CrossSubjectValidation_onDApreprocessed_as_Action/\" + training_description + \"_accuracies.txt\", \"a\")\n",
    "        print(\"Activity\\tAccuracy (% correct)\")\n",
    "#         log_file.write(\"Activity\\tAccuracy (% correct)\")\n",
    "        row_accuracies = [\"s\" + str(split), overall_acc]                                                                     \n",
    "        accuracy_dict = {}\n",
    "        for i in range(len(accuracies)):\n",
    "            activity = i + 1\n",
    "            accuracy = round( ((accuracies[i] / totals_per_class[i]) * 100), 2)\n",
    "            # Append this activities accuracy to file\n",
    "            accuracy_dict[activity] = accuracy\n",
    "            acc_report = activity_dict[activity] + \": \" + str(accuracy) + \"% correct\"\n",
    "            print(acc_report)\n",
    "    #         print(\"Total # samples of this class in test set:\", totals_per_class[i])\n",
    "#             log_file.write(acc_report)\n",
    "        \n",
    "        \n",
    "#         log_file.write(\"Overall accuracy: \" + str(overall_acc))\n",
    "\n",
    "#         log_file.close()\n",
    "\n",
    "        accuracy_across_splits.append(overall_acc)\n",
    "        for key in accuracy_dict.keys():                                                                     \n",
    "            row_accuracies.append(accuracy_dict[key])\n",
    "        writer.writerow(row_accuracies)                                                                     \n",
    "    ##################################################################################\n",
    "print(\"Accuracy averaged across all splits:\", np.mean(accuracy_across_splits))\n",
    "print(\"CONFUSION MATRIX\")\n",
    "cnf_matrix = confusion_matrix(testset_true, testset_pred)\n",
    "print(cm)\n",
    "# cp.dump(cm, open(\"confusion_matrix_\" + training_description + \".pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "\n",
    "# cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\"DailyActivity_ConfusionMatrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_csv(\"analysis_of_results_on_5_holdouts.csv\")\n",
    "\n",
    "# Compute the average accuracy for each activity\n",
    "print(\"Activity\\t\\tAverage accuracy\")\n",
    "for key in results_df.keys():\n",
    "    print(key, \"\\t\\t\", np.mean(results_df[key]), \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
